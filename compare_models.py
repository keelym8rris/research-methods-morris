"""
Model Comparison Script
=======================

This script runs ALL machine learning models and compares their performance.
Use this to quickly see which model performs best on your data.

Models compared:
1. Linear Regression
2. Decision Tree
3. Random Forest
4. Gradient Boosting
5. Neural Network
"""

import sys
import warnings
warnings.filterwarnings('ignore')
sys.path.append('.')

from src.data_loader import load_prostate_data, prepare_data
from src.evaluation import compare_models
from src.visualization import plot_model_comparison, plot_all_predictions_grid

# Import all model training functions
from src.models.linear_regression import train_linear_regression
from src.models.decision_tree import train_decision_tree
from src.models.random_forest import train_random_forest
from src.models.gradient_boosting import train_gradient_boosting
from src.models.neural_network import train_neural_network


def main():
    print("\n" + "="*70)
    print("ðŸ† COMPREHENSIVE MODEL COMPARISON")
    print("="*70)
    print("\nThis will train and evaluate 5 different ML models.")
    print("Grab a coffee, this may take a minute... â˜•\n")
    
    # Load and prepare data
    print("ðŸ“Š Step 1: Loading and preparing data...")
    print("-"*70)
    data = load_prostate_data()
    data_dict = prepare_data(data)
    
    # Train all models
    print("\nðŸ¤– Step 2: Training all models...")
    print("-"*70)
    
    models_results = []
    predictions_dict = {}
    
    # 1. Linear Regression
    print("\n1ï¸âƒ£ Training Linear Regression...")
    lr_model, lr_pred, lr_metrics = train_linear_regression(data_dict, verbose=False)
    models_results.append(lr_metrics)
    predictions_dict['Linear Regression'] = (lr_pred, lr_metrics['r2_score'])
    print(f"   âœ… Linear Regression: RÂ² = {lr_metrics['r2_score']:.4f}")
    
    # 2. Decision Tree
    print("\n2ï¸âƒ£ Training Decision Tree...")
    dt_model, dt_pred, dt_metrics = train_decision_tree(data_dict, verbose=False)
    models_results.append(dt_metrics)
    predictions_dict['Decision Tree'] = (dt_pred, dt_metrics['r2_score'])
    print(f"   âœ… Decision Tree: RÂ² = {dt_metrics['r2_score']:.4f}")
    
    # 3. Random Forest
    print("\n3ï¸âƒ£ Training Random Forest...")
    rf_model, rf_pred, rf_metrics = train_random_forest(data_dict, verbose=False)
    models_results.append(rf_metrics)
    predictions_dict['Random Forest'] = (rf_pred, rf_metrics['r2_score'])
    print(f"   âœ… Random Forest: RÂ² = {rf_metrics['r2_score']:.4f}")
    
    # 4. Gradient Boosting
    print("\n4ï¸âƒ£ Training Gradient Boosting...")
    gb_model, gb_pred, gb_metrics = train_gradient_boosting(data_dict, verbose=False)
    models_results.append(gb_metrics)
    predictions_dict['Gradient Boosting'] = (gb_pred, gb_metrics['r2_score'])
    print(f"   âœ… Gradient Boosting: RÂ² = {gb_metrics['r2_score']:.4f}")
    
    # 5. Neural Network
    print("\n5ï¸âƒ£ Training Neural Network...")
    nn_model, nn_pred, nn_metrics, nn_history = train_neural_network(data_dict, verbose=False)
    models_results.append(nn_metrics)
    predictions_dict['Neural Network'] = (nn_pred, nn_metrics['r2_score'])
    print(f"   âœ… Neural Network: RÂ² = {nn_metrics['r2_score']:.4f}")
    
    # Compare all models
    print("\n" + "="*70)
    print("ðŸ“Š Step 3: Comparing Model Performance")
    print("="*70)
    
    comparison_df = compare_models(models_results)
    print("\nðŸ† MODEL RANKINGS:")
    print(comparison_df.to_string(index=False))
    
    # Identify best model
    best_model = comparison_df.iloc[0]
    print("\n" + "="*70)
    print(f"ðŸ¥‡ WINNER: {best_model['model_name']}")
    print("="*70)
    print(f"\n   RÂ² Score:  {best_model['r2_score']:.4f} ({best_model['r2_score']*100:.1f}%)")
    print(f"   MSE:       {best_model['mse']:.4f}")
    print(f"   RMSE:      {best_model['rmse']:.4f}")
    print(f"   MAE:       {best_model['mae']:.4f}")
    print(f"\n   This model explains {best_model['r2_score']*100:.1f}% of PSA variance!")
    print("="*70)
    
    # Create comparison visualizations
    print("\nðŸ“Š Step 4: Creating comparison visualizations...")
    print("-"*70)
    
    print("\n1ï¸âƒ£ Creating model comparison charts...")
    plot_model_comparison(comparison_df, save=True)
    
    print("\n2ï¸âƒ£ Creating prediction comparison grid...")
    plot_all_predictions_grid(data_dict['y_test'], predictions_dict, save=True)
    
    # Summary
    print("\n" + "="*70)
    print("âœ… MODEL COMPARISON COMPLETE!")
    print("="*70)
    
    print("\nðŸ“Š Summary Statistics:")
    print(f"   â€¢ Best RÂ² Score:    {comparison_df['r2_score'].max():.4f}")
    print(f"   â€¢ Worst RÂ² Score:   {comparison_df['r2_score'].min():.4f}")
    print(f"   â€¢ Average RÂ² Score: {comparison_df['r2_score'].mean():.4f}")
    print(f"   â€¢ Best MSE:         {comparison_df['mse'].min():.4f}")
    
    print("\nðŸŽ¯ Recommendations:")
    if best_model['r2_score'] > 0.7:
        print(f"   â€¢ {best_model['model_name']} shows excellent performance!")
        print("   â€¢ Use this model for predictions")
    elif best_model['r2_score'] > 0.5:
        print(f"   â€¢ {best_model['model_name']} shows good performance")
        print("   â€¢ Consider hyperparameter tuning for improvement")
    else:
        print("   â€¢ All models show room for improvement")
        print("   â€¢ Consider feature engineering or more data")
    
    print("\nðŸ’¾ All visualizations saved to: results/figures/")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
