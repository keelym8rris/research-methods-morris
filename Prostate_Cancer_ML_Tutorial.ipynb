{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc6f62f",
   "metadata": {},
   "source": [
    "# üéì Your First Machine Learning Project: Prostate Cancer Analysis\n",
    "\n",
    "**Welcome!** This notebook is your friendly introduction to machine learning. Don't worry if you've never done this before - I'll explain everything step by step, like teaching a friend.\n",
    "\n",
    "## What We're Going To Do\n",
    "\n",
    "We're going to use **machine learning** to predict prostate cancer indicators from patient data. Think of machine learning as teaching a computer to find patterns in data - just like you might notice that \"when it's cloudy, it usually rains,\" machine learning helps computers find similar patterns in medical data.\n",
    "\n",
    "By the end of this tutorial, you'll:\n",
    "- ‚úÖ Understand what machine learning actually is\n",
    "- ‚úÖ Know how to explore and visualize medical data\n",
    "- ‚úÖ Build 5 different machine learning models\n",
    "- ‚úÖ Compare which models work best\n",
    "- ‚úÖ Understand what your results mean for prostate cancer research\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5eb483",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Section 1: Setting Up Your Environment\n",
    "\n",
    "First, we need to import **libraries** - these are like toolboxes that other programmers have built for us. Instead of writing everything from scratch, we can use their tools!\n",
    "\n",
    "### What each library does:\n",
    "- **pandas**: Helps us work with data in tables (like Excel)\n",
    "- **numpy**: Does mathematical calculations super fast\n",
    "- **matplotlib & seaborn**: Creates beautiful charts and graphs\n",
    "- **sklearn** (scikit-learn): The main machine learning library - has lots of pre-built models\n",
    "- **tensorflow**: Builds neural networks (we'll explain this later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # This hides warning messages to keep things clean\n",
    "\n",
    "# Data manipulation - working with tables of data\n",
    "import numpy as np  # For math and numbers\n",
    "import pandas as pd  # For working with data tables\n",
    "\n",
    "# Visualization - making charts and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning models and tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep Learning (Neural Networks)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Set styling for our graphs to make them look nice\n",
    "sns.set_theme(style='whitegrid', font='serif')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Make graphs a good size\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"üìä Using TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af9341",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Section 2: Understanding Your Data\n",
    "\n",
    "Now let's load our prostate cancer dataset and understand what we're working with.\n",
    "\n",
    "### What is this dataset?\n",
    "This is real medical data from prostate cancer patients. Each row represents one patient, and each column is a measurement or test result.\n",
    "\n",
    "### What do the column names mean?\n",
    "The names are medical abbreviations. Here's what they actually mean:\n",
    "\n",
    "- **lcavol**: Log of cancer volume (how big the tumor is)\n",
    "- **lweight**: Log of prostate weight\n",
    "- **age**: Patient's age in years\n",
    "- **lbph**: Log of benign prostatic hyperplasia amount (non-cancerous growth)\n",
    "- **svi**: Seminal vesicle invasion (0 = no, 1 = yes) - has cancer spread?\n",
    "- **lcp**: Log of capsular penetration (how far cancer has spread)\n",
    "- **gleason**: Gleason score (a grading system for prostate cancer, 6-10)\n",
    "- **pgg45**: Percentage of Gleason scores 4 or 5 (higher = more aggressive)\n",
    "- **lpsa**: Log of PSA level (**this is what we want to predict!**)\n",
    "- **train**: Whether this data point was used for training (we'll use this later)\n",
    "\n",
    "**PSA (Prostate-Specific Antigen)** is a protein that, when elevated, can indicate prostate cancer. Doctors use it as a screening tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/prostate.csv')\n",
    "\n",
    "print(\"üìà Dataset loaded successfully!\")\n",
    "print(f\"\\nüìä We have {len(data)} patients in our dataset\\n\")\n",
    "\n",
    "# Look at the first few rows\n",
    "print(\"First 5 patients:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c9fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about our data\n",
    "print(\"üìã Dataset Information:\\n\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "print(\"(This shows us the average, min, max, etc. for each column)\\n\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c6ce6",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Section 3: Data Exploration and Visualization\n",
    "\n",
    "Before we build any models, we need to **understand our data visually**. This is like getting to know someone before working with them!\n",
    "\n",
    "### Why visualize data?\n",
    "- üëÄ See patterns that numbers alone don't show\n",
    "- üîé Find unusual values (outliers)\n",
    "- üí° Understand relationships between variables\n",
    "- üìà Make informed decisions about which models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800309a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of our target variable (lpsa)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data['lpsa'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Log PSA Level')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Distribution of PSA Levels\\n(What we want to predict)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(data['lpsa'], vert=True)\n",
    "plt.ylabel('Log PSA Level')\n",
    "plt.title('PSA Levels - Box Plot\\n(Shows median and outliers)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä The histogram shows how PSA levels are distributed across patients.\")\n",
    "print(\"üì¶ The box plot shows the median (middle line) and any unusual values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation heatmap\n",
    "# This shows which variables are related to each other\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Calculate correlations (how variables relate to each other)\n",
    "# Values close to 1 = strong positive relationship\n",
    "# Values close to -1 = strong negative relationship\n",
    "# Values close to 0 = no relationship\n",
    "\n",
    "correlation_matrix = data.drop(columns=['train']).corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Heatmap\\n(How different measurements relate to each other)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüî• What to look for:\")\n",
    "print(\"  ‚Ä¢ Dark red = strong positive correlation (variables increase together)\")\n",
    "print(\"  ‚Ä¢ Dark blue = strong negative correlation (one increases, other decreases)\")\n",
    "print(\"  ‚Ä¢ White = no correlation (variables are independent)\")\n",
    "print(\"\\nüí° Look at the 'lpsa' row - these are variables that might help predict PSA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5341c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at relationships between specific features and PSA\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "features_to_plot = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'gleason']\n",
    "feature_names = ['Cancer Volume', 'Prostate Weight', 'Age', \n",
    "                 'BPH Amount', 'Seminal Vesicle Invasion', 'Gleason Score']\n",
    "\n",
    "for i, (feature, name) in enumerate(zip(features_to_plot, feature_names)):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].scatter(data[feature], data['lpsa'], alpha=0.6, color='coral')\n",
    "    axes[row, col].set_xlabel(name)\n",
    "    axes[row, col].set_ylabel('PSA Level')\n",
    "    axes[row, col].set_title(f'{name} vs PSA')\n",
    "    axes[row, col].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('How Different Factors Relate to PSA Levels', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Each dot is one patient. Look for patterns!\")\n",
    "print(\"   ‚Ä¢ Upward slope = as this factor increases, PSA tends to increase\")\n",
    "print(\"   ‚Ä¢ Downward slope = as this factor increases, PSA tends to decrease\")\n",
    "print(\"   ‚Ä¢ Random scatter = this factor doesn't strongly predict PSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b214c9",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Section 4: Preparing Data for Machine Learning\n",
    "\n",
    "Now we need to prepare our data for machine learning. This is like preparing ingredients before cooking!\n",
    "\n",
    "### Two important concepts:\n",
    "\n",
    "#### 1. **Train/Test Split**\n",
    "We split our data into two parts:\n",
    "- **Training data (80%)**: We use this to teach our model\n",
    "- **Testing data (20%)**: We use this to see how well our model learned\n",
    "\n",
    "**Why?** Imagine studying for a test. If the test has the exact same questions you studied, you can't tell if you truly understand the material or just memorized answers. Same with machine learning - we test on data the model hasn't seen before.\n",
    "\n",
    "#### 2. **Standardization (Scaling)**\n",
    "Different measurements have different scales:\n",
    "- Age: 40-80 years\n",
    "- PSA: 0-5 (on log scale)\n",
    "\n",
    "Some ML models get confused by this! Standardization puts everything on the same scale so the model can learn better. It's like converting all measurements to the same units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44000bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate our features (X) from our target (y)\n",
    "\n",
    "# Convert the 'train' column from TRUE/FALSE to 1/0\n",
    "data['train'] = data['train'].astype(int)\n",
    "\n",
    "# X = all columns except 'lpsa' (these are our inputs/features)\n",
    "X = data.drop(columns=['lpsa'])\n",
    "\n",
    "# y = only the 'lpsa' column (this is what we want to predict)\n",
    "y = data['lpsa']\n",
    "\n",
    "print(\"üìä Features (X):\")\n",
    "print(f\"   Shape: {X.shape} (meaning {X.shape[0]} patients, {X.shape[1]} features)\")\n",
    "print(f\"   Columns: {list(X.columns)}\\n\")\n",
    "\n",
    "print(\"üéØ Target (y):\")\n",
    "print(f\"   Shape: {y.shape} (meaning {y.shape[0]} PSA values)\")\n",
    "print(f\"   What we're predicting: PSA levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709486ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into training and testing sets\n",
    "# test_size=0.2 means 20% for testing, 80% for training\n",
    "# random_state=42 ensures we get the same split every time (for reproducibility)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÇÔ∏è Data Split Complete!\\n\")\n",
    "print(f\"üìö Training set: {len(X_train)} patients ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"üß™ Testing set: {len(X_test)} patients ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "print(\"\\nüí° We'll teach our model using the training set,\")\n",
    "print(\"   then test it on the testing set to see how well it learned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Standardize the features\n",
    "# This puts all features on the same scale\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler to transform testing data\n",
    "# (Important: we only FIT on training data, but TRANSFORM both!)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚öñÔ∏è Data Standardization Complete!\\n\")\n",
    "print(\"Before standardization (sample):\")\n",
    "print(X_train.iloc[0])\n",
    "print(\"\\nAfter standardization (same sample):\")\n",
    "print(X_train_scaled[0])\n",
    "print(\"\\nüí° Notice how the values are now smaller and on a similar scale!\")\n",
    "print(\"   Values around 0 with standard deviation of 1 - this helps ML models learn better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762e1c1",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® Section 5: Your First Model - Linear Regression\n",
    "\n",
    "Time to build your first machine learning model! üéâ\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Remember drawing a \"line of best fit\" in high school math? That's basically what linear regression does!\n",
    "\n",
    "**The idea:** Find the straight line (or in our case, a hyperplane since we have multiple variables) that best fits the data points. Then we can use this line to predict new values.\n",
    "\n",
    "**In simple terms:**\n",
    "- Linear regression tries to find the relationship between our features (age, cancer volume, etc.) and PSA levels\n",
    "- It creates an equation: `PSA = (weight1 √ó feature1) + (weight2 √ó feature2) + ... + constant`\n",
    "- The model \"learns\" the best weights to minimize prediction errors\n",
    "\n",
    "**When to use it:**\n",
    "- ‚úÖ Great for understanding which features are important\n",
    "- ‚úÖ Fast to train and easy to interpret\n",
    "- ‚úÖ Works well when relationships are roughly linear\n",
    "- ‚ùå Not good for complex, non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Linear Regression model\n",
    "\n",
    "print(\"üèóÔ∏è Building Linear Regression model...\\n\")\n",
    "\n",
    "# Step 1: Initialize (create) the model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Step 2: Train the model (this is where the \"learning\" happens!)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Model trained successfully!\\n\")\n",
    "\n",
    "# Let's see what the model learned\n",
    "print(\"üìä Feature Importance (Coefficients):\")\n",
    "print(\"(Larger absolute values = more important for prediction)\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ Positive coefficient = as this increases, PSA tends to increase\")\n",
    "print(\"   ‚Ä¢ Negative coefficient = as this increases, PSA tends to decrease\")\n",
    "print(\"   ‚Ä¢ Larger absolute value = stronger influence on PSA prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance in Linear Regression Model\\n(Green = increases PSA, Red = decreases PSA)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"üîÆ Making predictions on test data...\\n\")\n",
    "print(\"Sample predictions vs actual values:\\n\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual PSA': y_test[:10].values,\n",
    "    'Predicted PSA': y_pred_lr[:10],\n",
    "    'Difference': y_test[:10].values - y_pred_lr[:10]\n",
    "})\n",
    "\n",
    "print(comparison_df)\n",
    "print(\"\\nüí° The model is trying to predict the actual PSA values!\")\n",
    "print(\"   The closer 'Predicted' is to 'Actual', the better the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.6, s=100, edgecolors='black', linewidth=1)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction Line')\n",
    "plt.xlabel('Actual PSA Levels', fontsize=12)\n",
    "plt.ylabel('Predicted PSA Levels', fontsize=12)\n",
    "plt.title('Linear Regression: Predicted vs Actual PSA Levels', fontsize=14, pad=20)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä If our model was perfect, all points would be on the red line!\")\n",
    "print(\"   Points close to the line = good predictions\")\n",
    "print(\"   Points far from the line = poor predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9072fc",
   "metadata": {},
   "source": [
    "---\n",
    "## üìè Section 6: Understanding Model Performance\n",
    "\n",
    "How do we know if our model is good? We need metrics (measurements)!\n",
    "\n",
    "### Two Key Metrics:\n",
    "\n",
    "#### 1. **Mean Squared Error (MSE)**\n",
    "- Measures the average squared difference between predicted and actual values\n",
    "- **Lower is better!** (0 = perfect predictions)\n",
    "- Think of it as: \"On average, how wrong are my predictions?\"\n",
    "- Formula: Average of (actual - predicted)¬≤\n",
    "\n",
    "#### 2. **R¬≤ Score (R-squared)**\n",
    "- Tells you what percentage of the variance in PSA your model explains\n",
    "- **Ranges from 0 to 1** (1 = perfect, 0 = no better than guessing the average)\n",
    "- Example: R¬≤ = 0.75 means your model explains 75% of the variation in PSA levels\n",
    "- Think of it as: \"How well does my model capture the patterns in the data?\"\n",
    "\n",
    "### Rule of Thumb:\n",
    "- R¬≤ > 0.7: Great model! üåü\n",
    "- R¬≤ = 0.5-0.7: Good model ‚úÖ\n",
    "- R¬≤ < 0.5: Needs improvement üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dac0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics for Linear Regression\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä LINEAR REGRESSION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìâ Mean Squared Error (MSE): {mse_lr:.4f}\")\n",
    "print(f\"   ‚Üí Lower is better. This is the average squared error.\")\n",
    "print(f\"\\nüìà R¬≤ Score: {r2_lr:.4f} ({r2_lr*100:.1f}%)\")\n",
    "print(f\"   ‚Üí Our model explains {r2_lr*100:.1f}% of the variance in PSA levels!\")\n",
    "\n",
    "if r2_lr > 0.7:\n",
    "    print(\"\\nüåü Excellent! This is a strong model.\")\n",
    "elif r2_lr > 0.5:\n",
    "    print(\"\\n‚úÖ Good! This model captures meaningful patterns.\")\n",
    "else:\n",
    "    print(\"\\nüìà Room for improvement. Let's try other models!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc144892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the errors (residuals)\n",
    "residuals = y_test - y_pred_lr\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Residual plot\n",
    "axes[0].scatter(y_pred_lr, residuals, alpha=0.6, s=80)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted PSA', fontsize=11)\n",
    "axes[0].set_ylabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "axes[0].set_title('Residual Plot\\n(Points should be randomly scattered around zero)', fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution of errors\n",
    "axes[1].hist(residuals, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residual (Error)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Distribution of Prediction Errors\\n(Should look like a bell curve centered at 0)', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° What to look for:\")\n",
    "print(\"   ‚Ä¢ Left plot: Random scatter = good! Pattern = model is missing something\")\n",
    "print(\"   ‚Ä¢ Right plot: Bell curve centered at 0 = good predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b202d",
   "metadata": {},
   "source": [
    "---\n",
    "## üå≥ Section 7: Decision Trees - A Different Approach\n",
    "\n",
    "Now let's try a completely different type of model: **Decision Trees**!\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "Think of a decision tree like a flowchart that you might use to make a decision. It asks a series of yes/no questions:\n",
    "\n",
    "```\n",
    "Is cancer volume > 2.5?\n",
    "‚îú‚îÄ YES ‚Üí Is patient age > 65?\n",
    "‚îÇ        ‚îú‚îÄ YES ‚Üí Predict HIGH PSA\n",
    "‚îÇ        ‚îî‚îÄ NO ‚Üí Predict MEDIUM PSA\n",
    "‚îî‚îÄ NO ‚Üí Predict LOW PSA\n",
    "```\n",
    "\n",
    "### How it works:\n",
    "1. The model looks at all features and finds the best question to split the data\n",
    "2. It keeps splitting until it creates groups that are very similar\n",
    "3. For each group, it predicts the average PSA value\n",
    "\n",
    "### Pros and Cons:\n",
    "‚úÖ **Pros:**\n",
    "- Very easy to understand and explain\n",
    "- Can capture non-linear relationships (curves in data)\n",
    "- Works with both numbers and categories\n",
    "\n",
    "‚ùå **Cons:**\n",
    "- Can easily \"overfit\" (memorize training data instead of learning patterns)\n",
    "- Sometimes unstable (small data changes = big model changes)\n",
    "- May not perform as well as other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd79c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Decision Tree model\n",
    "\n",
    "print(\"üå≥ Building Decision Tree model...\\n\")\n",
    "\n",
    "# Create the model (random_state ensures reproducibility)\n",
    "dt_model = DecisionTreeRegressor(random_state=42, max_depth=5)  # max_depth prevents overfitting\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"‚úÖ Decision Tree trained!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DECISION TREE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìâ Mean Squared Error (MSE): {mse_dt:.4f}\")\n",
    "print(f\"üìà R¬≤ Score: {r2_dt:.4f} ({r2_dt*100:.1f}%)\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c25e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance for Decision Tree\n",
    "# This shows which features the tree uses most for making decisions\n",
    "\n",
    "feature_imp_dt = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_dt['Feature'], feature_imp_dt['Importance'], color='forestgreen', alpha=0.7)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Decision Tree: Feature Importance\\n(Which features does the tree use most?)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Feature Importance Rankings:\\n\")\n",
    "print(feature_imp_dt)\n",
    "print(\"\\nüí° The tree uses these features most for splitting decisions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c55679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions: Decision Tree vs Linear Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Linear Regression\n",
    "axes[0].scatter(y_test, y_pred_lr, alpha=0.6, s=80, color='blue', edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual PSA', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted PSA', fontsize=11)\n",
    "axes[0].set_title(f'Linear Regression\\nR¬≤ = {r2_lr:.3f}', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "axes[1].scatter(y_test, y_pred_dt, alpha=0.6, s=80, color='green', edgecolors='black', linewidth=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual PSA', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted PSA', fontsize=11)\n",
    "axes[1].set_title(f'Decision Tree\\nR¬≤ = {r2_dt:.3f}', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Comparison: Actual vs Predicted PSA', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c6944",
   "metadata": {},
   "source": [
    "---\n",
    "## üå≤ Section 8: Random Forests - Combining Multiple Trees\n",
    "\n",
    "If one decision tree is good, what about many trees working together? That's a **Random Forest**!\n",
    "\n",
    "### What is a Random Forest?\n",
    "\n",
    "Imagine asking 100 doctors for their diagnosis instead of just 1. You'd probably trust the majority opinion more than a single doctor's opinion. Random Forest works the same way:\n",
    "\n",
    "1. **Creates many decision trees** (typically 100+)\n",
    "2. Each tree is trained on a slightly different subset of the data\n",
    "3. Each tree makes a prediction\n",
    "4. **Final prediction = average of all tree predictions**\n",
    "\n",
    "This is called **Ensemble Learning** - combining multiple models to make better predictions!\n",
    "\n",
    "### Why Random Forests are Great:\n",
    "‚úÖ Usually more accurate than a single decision tree\n",
    "‚úÖ Less prone to overfitting\n",
    "‚úÖ Handles non-linear relationships well\n",
    "‚úÖ Can tell you which features are most important\n",
    "‚úÖ Works well \"out of the box\" without much tuning\n",
    "\n",
    "### Downsides:\n",
    "‚ùå Slower to train (building many trees takes time)\n",
    "‚ùå Harder to interpret than a single decision tree\n",
    "‚ùå Uses more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2019cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Random Forest model\n",
    "\n",
    "print(\"üå≤üå≤üå≤ Building Random Forest (100 trees)...\\n\")\n",
    "\n",
    "# Create the model with 100 trees\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42,\n",
    "    max_depth=10,      # Maximum depth of each tree\n",
    "    n_jobs=-1          # Use all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"‚úÖ Random Forest trained with 100 trees!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä RANDOM FOREST PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìâ Mean Squared Error (MSE): {mse_rf:.4f}\")\n",
    "print(f\"üìà R¬≤ Score: {r2_rf:.4f} ({r2_rf*100:.1f}%)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare to previous models\n",
    "print(\"\\nüìä Comparison so far:\")\n",
    "print(f\"   Linear Regression: R¬≤ = {r2_lr:.4f}\")\n",
    "print(f\"   Decision Tree:     R¬≤ = {r2_dt:.4f}\")\n",
    "print(f\"   Random Forest:     R¬≤ = {r2_rf:.4f} ‚Üê Latest model\")\n",
    "\n",
    "best_model = max([('Linear Regression', r2_lr), ('Decision Tree', r2_dt), ('Random Forest', r2_rf)], \n",
    "                 key=lambda x: x[1])\n",
    "print(f\"\\nüèÜ Best model so far: {best_model[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9665bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_imp_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_rf['Feature'], feature_imp_rf['Importance'], \n",
    "         color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Random Forest: Feature Importance\\n(Averaged across all 100 trees)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Top 5 Most Important Features:\\n\")\n",
    "print(feature_imp_rf.head())\n",
    "print(\"\\nüí° Random Forest uses these features most for predictions!\")\n",
    "print(\"   This can give us insights into what drives PSA levels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29384461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_rf, alpha=0.6, s=100, color='darkgreen', \n",
    "            edgecolors='black', linewidth=1, label='Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction Line')\n",
    "plt.xlabel('Actual PSA Levels', fontsize=12)\n",
    "plt.ylabel('Predicted PSA Levels', fontsize=12)\n",
    "plt.title(f'Random Forest: Predicted vs Actual PSA\\nR¬≤ = {r2_rf:.4f}', \n",
    "          fontsize=14, pad=20)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0251c",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Section 9: Gradient Boosting - Sequential Learning\n",
    "\n",
    "Now let's try another ensemble method: **Gradient Boosting**!\n",
    "\n",
    "### What is Gradient Boosting?\n",
    "\n",
    "While Random Forest builds many trees independently and averages them, Gradient Boosting builds trees **sequentially**, where each new tree tries to fix the mistakes of the previous trees.\n",
    "\n",
    "**The Process:**\n",
    "1. Build tree #1 ‚Üí Make predictions ‚Üí Calculate errors\n",
    "2. Build tree #2 to predict those errors ‚Üí Combine with tree #1\n",
    "3. Build tree #3 to predict remaining errors ‚Üí Combine with trees #1 and #2\n",
    "4. Keep going until errors are minimized!\n",
    "\n",
    "**Analogy:** Imagine studying for a test:\n",
    "- Tree 1: You study and take a practice test\n",
    "- Tree 2: You focus on questions you got wrong\n",
    "- Tree 3: You focus on questions you're still getting wrong\n",
    "- Each iteration makes you better at the hardest problems!\n",
    "\n",
    "### Pros and Cons:\n",
    "‚úÖ **Pros:**\n",
    "- Often very accurate (wins many ML competitions!)\n",
    "- Good at handling complex patterns\n",
    "- Provides feature importance\n",
    "\n",
    "‚ùå **Cons:**\n",
    "- Can be slow to train (trees built sequentially, not in parallel)\n",
    "- More prone to overfitting than Random Forest\n",
    "- Requires careful tuning of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450647b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Gradient Boosting model\n",
    "\n",
    "print(\"üöÄ Building Gradient Boosting model...\\n\")\n",
    "print(\"(This learns sequentially, so it might take a moment...)\\n\")\n",
    "\n",
    "# Create the model\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,      # Number of boosting stages\n",
    "    learning_rate=0.1,     # How much each tree contributes\n",
    "    max_depth=3,           # Depth of each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"‚úÖ Gradient Boosting trained!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"üìä GRADIENT BOOSTING PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìâ Mean Squared Error (MSE): {mse_gb:.4f}\")\n",
    "print(f\"üìà R¬≤ Score: {r2_gb:.4f} ({r2_gb*100:.1f}%)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare all models so far\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(f\"   Linear Regression:  R¬≤ = {r2_lr:.4f}\")\n",
    "print(f\"   Decision Tree:      R¬≤ = {r2_dt:.4f}\")\n",
    "print(f\"   Random Forest:      R¬≤ = {r2_rf:.4f}\")\n",
    "print(f\"   Gradient Boosting:  R¬≤ = {r2_gb:.4f} ‚Üê Latest model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Gradient Boosting\n",
    "feature_imp_gb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_imp_gb['Feature'], feature_imp_gb['Importance'], \n",
    "         color='orange', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Gradient Boosting: Feature Importance', fontsize=14, pad=20)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Feature Importance Rankings:\\n\")\n",
    "print(feature_imp_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1451dbf",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† Section 10: Introduction to Neural Networks (Deep Learning)\n",
    "\n",
    "Now for something exciting - **Neural Networks**! This is the foundation of \"deep learning.\"\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "Neural networks are loosely inspired by how our brains work. Just like your brain has billions of neurons connected together, a neural network has artificial \"neurons\" organized in layers.\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer\n",
    "[Features]  ‚Üí  [Processing]  ‚Üí [Prediction]\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "1. **Input Layer**: Takes in your features (age, cancer volume, etc.)\n",
    "2. **Hidden Layers**: Process the information through multiple \"neurons\"\n",
    "   - Each neuron takes inputs, applies weights, and passes through an activation function\n",
    "   - This allows the network to learn complex, non-linear patterns\n",
    "3. **Output Layer**: Produces the final prediction (PSA level)\n",
    "\n",
    "**Learning Process (Training):**\n",
    "1. Make a prediction\n",
    "2. Calculate how wrong you were (loss)\n",
    "3. Adjust the weights to reduce the error\n",
    "4. Repeat thousands of times!\n",
    "\n",
    "This process is called **backpropagation** - the network literally learns from its mistakes!\n",
    "\n",
    "### Why Neural Networks?\n",
    "‚úÖ Can learn incredibly complex patterns\n",
    "‚úÖ State-of-the-art for images, text, speech\n",
    "‚úÖ Can combine different types of data\n",
    "\n",
    "‚ùå Need more data to train well\n",
    "‚ùå Harder to interpret (\"black box\")\n",
    "‚ùå Require more computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb11e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Neural Network with TensorFlow/Keras\n",
    "\n",
    "print(\"üß† Building a Neural Network...\\n\")\n",
    "\n",
    "# Define the architecture\n",
    "nn_model = Sequential([\n",
    "    # Input layer automatically inferred from first Dense layer\n",
    "    \n",
    "    # Hidden Layer 1: 64 neurons with ReLU activation\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    \n",
    "    # Hidden Layer 2: 32 neurons with ReLU activation\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Hidden Layer 3: 16 neurons with ReLU activation\n",
    "    Dense(16, activation='relu'),\n",
    "    \n",
    "    # Output Layer: 1 neuron (for regression - predicting a number)\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "print(\"üèóÔ∏è Neural Network Architecture:\")\n",
    "print(\"=\"*60)\n",
    "nn_model.summary()\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüí° Understanding the architecture:\")\n",
    "print(\"   ‚Ä¢ Input: 10 features (age, cancer volume, etc.)\")\n",
    "print(\"   ‚Ä¢ Hidden Layer 1: 64 neurons processing the input\")\n",
    "print(\"   ‚Ä¢ Hidden Layer 2: 32 neurons processing layer 1\")\n",
    "print(\"   ‚Ä¢ Hidden Layer 3: 16 neurons processing layer 2\")\n",
    "print(\"   ‚Ä¢ Output: 1 neuron giving the PSA prediction\")\n",
    "print(\"\\n   Total parameters: These are the 'weights' the network will learn!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model (set up how it will learn)\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer='adam',              # Algorithm for adjusting weights\n",
    "    loss='mean_squared_error'      # How we measure errors\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Model compiled and ready to train!\\n\")\n",
    "print(\"Configuration:\")\n",
    "print(\"   ‚Ä¢ Optimizer: Adam (adaptive learning rate)\")\n",
    "print(\"   ‚Ä¢ Loss function: Mean Squared Error\")\n",
    "print(\"\\n   Now let's train the network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "\n",
    "print(\"üéì Training the Neural Network...\\n\")\n",
    "print(\"(The network will go through the training data 100 times)\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,              # Number of times to go through the entire dataset\n",
    "    batch_size=32,           # Number of samples per gradient update\n",
    "    validation_split=0.2,    # Use 20% of training data for validation\n",
    "    verbose=0                # Don't print every epoch (keeps output clean)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training complete!\\n\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training process\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch (Training Iteration)', fontsize=12)\n",
    "plt.ylabel('Loss (Mean Squared Error)', fontsize=12)\n",
    "plt.title('Neural Network Training Progress\\n(Loss should decrease over time)', \n",
    "          fontsize=14, pad=20)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìâ What this graph shows:\")\n",
    "print(\"   ‚Ä¢ Both lines going down = network is learning!\")\n",
    "print(\"   ‚Ä¢ Training loss < validation loss = normal\")\n",
    "print(\"   ‚Ä¢ If validation loss goes up while training goes down = overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate the neural network\n",
    "\n",
    "y_pred_nn = nn_model.predict(X_test_scaled, verbose=0)\n",
    "y_pred_nn = y_pred_nn.flatten()  # Convert from 2D to 1D array\n",
    "\n",
    "# Calculate metrics\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä NEURAL NETWORK PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìâ Mean Squared Error (MSE): {mse_nn:.4f}\")\n",
    "print(f\"üìà R¬≤ Score: {r2_nn:.4f} ({r2_nn*100:.1f}%)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with all previous models\n",
    "print(\"\\nüìä ALL MODELS COMPARISON:\")\n",
    "print(f\"   Linear Regression:  R¬≤ = {r2_lr:.4f}\")\n",
    "print(f\"   Decision Tree:      R¬≤ = {r2_dt:.4f}\")\n",
    "print(f\"   Random Forest:      R¬≤ = {r2_rf:.4f}\")\n",
    "print(f\"   Gradient Boosting:  R¬≤ = {r2_gb:.4f}\")\n",
    "print(f\"   Neural Network:     R¬≤ = {r2_nn:.4f} ‚Üê Latest model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Neural Network predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_nn, alpha=0.6, s=100, color='purple', \n",
    "            edgecolors='black', linewidth=1, label='Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction Line')\n",
    "plt.xlabel('Actual PSA Levels', fontsize=12)\n",
    "plt.ylabel('Predicted PSA Levels', fontsize=12)\n",
    "plt.title(f'Neural Network: Predicted vs Actual PSA\\nR¬≤ = {r2_nn:.4f}', \n",
    "          fontsize=14, pad=20)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e957c00",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÜ Section 11: Comparing All Models\n",
    "\n",
    "Now let's put all our models side by side and see which one performs best!\n",
    "\n",
    "We've built 5 different models:\n",
    "1. **Linear Regression** - Simple, interpretable baseline\n",
    "2. **Decision Tree** - Rule-based, easy to visualize\n",
    "3. **Random Forest** - Ensemble of trees\n",
    "4. **Gradient Boosting** - Sequential ensemble learning\n",
    "5. **Neural Network** - Deep learning approach\n",
    "\n",
    "Let's compare them using our two key metrics: **MSE** (lower is better) and **R¬≤** (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', \n",
    "              'Gradient Boosting', 'Neural Network'],\n",
    "    'MSE': [mse_lr, mse_dt, mse_rf, mse_gb, mse_nn],\n",
    "    'R¬≤ Score': [r2_lr, r2_dt, r2_rf, r2_gb, r2_nn]\n",
    "})\n",
    "\n",
    "# Sort by R¬≤ score (descending)\n",
    "model_comparison = model_comparison.sort_values('R¬≤ Score', ascending=False)\n",
    "model_comparison['Rank'] = range(1, len(model_comparison) + 1)\n",
    "model_comparison = model_comparison[['Rank', 'Model', 'MSE', 'R¬≤ Score']]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üèÜ MODEL PERFORMANCE RANKINGS\")\n",
    "print(\"=\"*70)\n",
    "print(model_comparison.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_model = model_comparison.iloc[0]\n",
    "print(f\"\\nü•á WINNER: {best_model['Model']}\")\n",
    "print(f\"   R¬≤ Score: {best_model['R¬≤ Score']:.4f}\")\n",
    "print(f\"   MSE: {best_model['MSE']:.4f}\")\n",
    "print(f\"\\n   This model explains {best_model['R¬≤ Score']*100:.1f}% of the variance in PSA levels!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712aff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison - MSE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: MSE Comparison\n",
    "colors_mse = ['gold' if i == 0 else 'silver' if i == 1 else 'chocolate' if i == 2 else 'lightgray' \n",
    "              for i in range(len(model_comparison))]\n",
    "bars1 = axes[0].barh(model_comparison['Model'], model_comparison['MSE'], \n",
    "                     color=colors_mse, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Mean Squared Error (Lower is Better)', fontsize=12)\n",
    "axes[0].set_ylabel('Model', fontsize=12)\n",
    "axes[0].set_title('Model Comparison: Mean Squared Error (MSE)', fontsize=13, pad=15)\n",
    "axes[0].grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    axes[0].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=10, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot 2: R¬≤ Comparison\n",
    "model_comparison_sorted_r2 = model_comparison.sort_values('R¬≤ Score', ascending=True)\n",
    "colors_r2 = ['gold' if x == model_comparison_sorted_r2['R¬≤ Score'].max() \n",
    "             else 'silver' if x == model_comparison_sorted_r2['R¬≤ Score'].iloc[-2] \n",
    "             else 'chocolate' if x == model_comparison_sorted_r2['R¬≤ Score'].iloc[-3] \n",
    "             else 'lightgray' \n",
    "             for x in model_comparison_sorted_r2['R¬≤ Score']]\n",
    "bars2 = axes[1].barh(model_comparison_sorted_r2['Model'], \n",
    "                     model_comparison_sorted_r2['R¬≤ Score'], \n",
    "                     color=colors_r2, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('R¬≤ Score (Higher is Better)', fontsize=12)\n",
    "axes[1].set_ylabel('Model', fontsize=12)\n",
    "axes[1].set_title('Model Comparison: R-squared (R¬≤)', fontsize=13, pad=15)\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    axes[1].text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=10,\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.suptitle('üèÜ Final Model Comparison\\n(Gold = 1st, Silver = 2nd, Bronze = 3rd)', \n",
    "             fontsize=15, y=1.02, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side prediction comparison for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.delaxes(axes[1, 2])  # Remove the 6th subplot\n",
    "\n",
    "models_data = [\n",
    "    ('Linear Regression', y_pred_lr, r2_lr, 'blue'),\n",
    "    ('Decision Tree', y_pred_dt, r2_dt, 'green'),\n",
    "    ('Random Forest', y_pred_rf, r2_rf, 'darkgreen'),\n",
    "    ('Gradient Boosting', y_pred_gb, r2_gb, 'orange'),\n",
    "    ('Neural Network', y_pred_nn, r2_nn, 'purple')\n",
    "]\n",
    "\n",
    "for idx, (name, predictions, r2, color) in enumerate(models_data):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    axes[row, col].scatter(y_test, predictions, alpha=0.6, s=60, \n",
    "                          color=color, edgecolors='black', linewidth=0.5)\n",
    "    axes[row, col].plot([y_test.min(), y_test.max()], \n",
    "                        [y_test.min(), y_test.max()], \n",
    "                        'r--', lw=2, alpha=0.8)\n",
    "    axes[row, col].set_xlabel('Actual PSA', fontsize=10)\n",
    "    axes[row, col].set_ylabel('Predicted PSA', fontsize=10)\n",
    "    axes[row, col].set_title(f'{name}\\nR¬≤ = {r2:.4f}', fontsize=11, weight='bold')\n",
    "    axes[row, col].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('All Models: Actual vs Predicted PSA Levels', fontsize=16, y=0.995, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Quick Analysis:\")\n",
    "print(\"   ‚Ä¢ Look at how tightly the points cluster around the red line\")\n",
    "print(\"   ‚Ä¢ Tighter clustering = better predictions\")\n",
    "print(\"   ‚Ä¢ Compare the R¬≤ scores to see which model performs best overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5f907",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Section 12: What the Results Mean\n",
    "\n",
    "Now let's interpret what we've learned from a **medical research perspective**.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### 1. **Model Performance**\n",
    "Our models were able to predict PSA levels with varying degrees of accuracy. The best model achieved an R¬≤ score of around 0.75, meaning it explains about 75% of the variation in PSA levels. This is quite good for medical data!\n",
    "\n",
    "#### 2. **Most Important Features**\n",
    "Across different models, certain features consistently appeared as important:\n",
    "- **Cancer volume (lcavol)**: Usually the strongest predictor\n",
    "- **Capsular penetration (lcp)**: How far cancer has spread\n",
    "- **Weight/Size factors**: Prostate weight matters\n",
    "\n",
    "This aligns with medical knowledge - larger tumors and more aggressive spread correlate with higher PSA.\n",
    "\n",
    "#### 3. **Clinical Implications**\n",
    "- These models could potentially **assist doctors** in:\n",
    "  - Screening decisions (who needs further testing?)\n",
    "  - Risk stratification (how aggressive is the cancer?)\n",
    "  - Treatment planning (what interventions are needed?)\n",
    "\n",
    "### Important Limitations:\n",
    "\n",
    "‚ö†Ô∏è **This is a learning exercise, not a medical tool!** Real clinical applications require:\n",
    "- Much larger datasets (thousands of patients, not 97)\n",
    "- Rigorous validation on independent datasets\n",
    "- Clinical trials and regulatory approval\n",
    "- Expert medical oversight\n",
    "- Consideration of ethical and bias issues\n",
    "\n",
    "### What We Learned About Machine Learning:\n",
    "\n",
    "1. **No single \"best\" model**: Different models have different strengths\n",
    "2. **Trade-offs matter**: \n",
    "   - Linear Regression: Simple, interpretable, fast\n",
    "   - Neural Networks: Powerful, complex, harder to interpret\n",
    "3. **Data quality is crucial**: Better data > fancier models\n",
    "4. **Validation is essential**: Always test on unseen data\n",
    "\n",
    "### For Your Research:\n",
    "\n",
    "You now have a foundation to:\n",
    "- Explore larger prostate cancer datasets\n",
    "- Experiment with different features and models\n",
    "- Compare tabular (table) data approaches with image-based approaches\n",
    "- Write your research abstract with confidence about methodologies\n",
    "- Understand machine learning vs deep learning trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final summary visualization\n",
    "print(\"=\"*70)\n",
    "print(\"üìä FINAL SUMMARY: YOUR MACHINE LEARNING JOURNEY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéì What You Accomplished:\\n\")\n",
    "print(\"   ‚úÖ Loaded and explored a real medical dataset\")\n",
    "print(\"   ‚úÖ Visualized patterns and relationships in the data\")\n",
    "print(\"   ‚úÖ Built 5 different machine learning models:\")\n",
    "print(\"      ‚Ä¢ Linear Regression (classical ML)\")\n",
    "print(\"      ‚Ä¢ Decision Tree (rule-based ML)\")\n",
    "print(\"      ‚Ä¢ Random Forest (ensemble ML)\")\n",
    "print(\"      ‚Ä¢ Gradient Boosting (sequential ensemble ML)\")\n",
    "print(\"      ‚Ä¢ Neural Network (deep learning)\")\n",
    "print(\"   ‚úÖ Evaluated and compared model performance\")\n",
    "print(\"   ‚úÖ Identified important features for PSA prediction\")\n",
    "print(\"\\nüèÜ Best Performing Model:\\n\")\n",
    "print(f\"   {best_model['Model']}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Score: {best_model['R¬≤ Score']:.4f}\")\n",
    "print(f\"   ‚Ä¢ MSE: {best_model['MSE']:.4f}\")\n",
    "print(\"\\nüí° Key Insights:\\n\")\n",
    "print(\"   ‚Ä¢ Cancer volume is the strongest predictor of PSA levels\")\n",
    "print(\"   ‚Ä¢ Ensemble methods (Random Forest, Gradient Boosting) often perform well\")\n",
    "print(\"   ‚Ä¢ Even simple models (Linear Regression) can be effective\")\n",
    "print(\"   ‚Ä¢ Model selection depends on your goals (accuracy vs interpretability)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f0850",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Section 13: Next Steps in Your Learning Journey\n",
    "\n",
    "Congratulations! You've completed your first machine learning project! üéâ\n",
    "\n",
    "But this is just the beginning. Here's what you can do next:\n",
    "\n",
    "### üî¨ Immediate Next Steps (For Your Research):\n",
    "\n",
    "#### 1. **Work with Larger Datasets**\n",
    "- Try the other Kaggle dataset your advisor mentioned\n",
    "- Explore datasets with more features (lifestyle factors, genetic markers, etc.)\n",
    "- Apply these same techniques to see if patterns hold\n",
    "\n",
    "#### 2. **Write Your Abstract**\n",
    "You now understand:\n",
    "- What machine learning is and how it works\n",
    "- Different ML approaches (classical ML vs deep learning)\n",
    "- How to evaluate and compare models\n",
    "- The potential and limitations of ML in prostate cancer research\n",
    "\n",
    "Use this knowledge to write about methodologies in your abstract!\n",
    "\n",
    "#### 3. **Explore Image-Based ML**\n",
    "- Learn about CNNs (Convolutional Neural Networks) for medical imaging\n",
    "- Look into prostate MRI analysis\n",
    "- Explore how imaging + clinical data can be combined\n",
    "\n",
    "### üìö Deepen Your ML Knowledge:\n",
    "\n",
    "#### **Beginner-Friendly Resources:**\n",
    "1. **Books:**\n",
    "   - \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n",
    "   - \"Python Data Science Handbook\" by Jake VanderPlas\n",
    "\n",
    "2. **Online Courses:**\n",
    "   - Andrew Ng's Machine Learning course (Coursera)\n",
    "   - Fast.ai courses (practical deep learning)\n",
    "   - Google's Machine Learning Crash Course\n",
    "\n",
    "3. **Practice Platforms:**\n",
    "   - Kaggle (competitions and datasets)\n",
    "   - UCI Machine Learning Repository\n",
    "   - Papers with Code (see latest research)\n",
    "\n",
    "### üîß Skills to Develop:\n",
    "\n",
    "#### **Technical Skills:**\n",
    "- **Feature Engineering**: Creating better input features\n",
    "- **Hyperparameter Tuning**: Optimizing model settings\n",
    "- **Cross-Validation**: Better model evaluation\n",
    "- **Handling Imbalanced Data**: Common in medical datasets\n",
    "- **Ensemble Methods**: Combining multiple models\n",
    "\n",
    "#### **Domain Skills:**\n",
    "- **Medical Data Understanding**: Learn more about prostate cancer biology\n",
    "- **Research Methods**: Study design, bias, validation\n",
    "- **Ethics in ML**: Fairness, privacy, interpretability in healthcare\n",
    "\n",
    "### üéØ Project Ideas:\n",
    "\n",
    "1. **Beginner:**\n",
    "   - Add more features to this analysis\n",
    "   - Try different train/test splits\n",
    "   - Experiment with model hyperparameters\n",
    "\n",
    "2. **Intermediate:**\n",
    "   - Build a classification model (cancer vs no cancer)\n",
    "   - Create feature interactions (combining variables)\n",
    "   - Try more advanced models (XGBoost, LightGBM)\n",
    "\n",
    "3. **Advanced:**\n",
    "   - Multi-task learning (predict multiple outcomes)\n",
    "   - Time-series analysis (patient progression over time)\n",
    "   - Combine imaging and clinical data\n",
    "\n",
    "### üí≠ Things to Remember:\n",
    "\n",
    "1. **Start simple, then get complex**: Master basics before diving into advanced techniques\n",
    "2. **Focus on understanding, not just coding**: Know *why* methods work, not just *how*\n",
    "3. **Practice regularly**: Build small projects frequently\n",
    "4. **Learn from failures**: Model didn't work? That's valuable data!\n",
    "5. **Stay curious**: ML is evolving rapidly - keep learning\n",
    "\n",
    "### üìù For Your Abstract Deadline:\n",
    "\n",
    "Use what you learned here to:\n",
    "- Describe the prostate cancer dataset\n",
    "- Explain ML and deep learning approaches\n",
    "- Discuss potential applications and challenges\n",
    "- Outline your research methodology\n",
    "\n",
    "You're now equipped to speak confidently about machine learning in your research! üåü\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Thoughts\n",
    "\n",
    "Machine learning is a powerful tool, but it's just that - a **tool**. The real power comes from:\n",
    "- Asking good research questions\n",
    "- Understanding your data deeply\n",
    "- Interpreting results in context\n",
    "- Collaborating with domain experts (doctors, researchers)\n",
    "- Maintaining ethical standards\n",
    "\n",
    "You've taken the first step on an exciting journey. Keep learning, stay curious, and remember:\n",
    "\n",
    "**Every expert was once a beginner!** üöÄ\n",
    "\n",
    "Good luck with your research, Keely! Feel free to reach out if you have questions as you continue learning.\n",
    "\n",
    "---\n",
    "\n",
    "*Happy Learning! üìäüß†üíª*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bac686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final encouragement!\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ú® CONGRATULATIONS! ‚ú®\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou've completed your first machine learning project!\")\n",
    "print(\"\\nüéì You now understand:\")\n",
    "print(\"   ‚Ä¢ What machine learning is and how it works\")\n",
    "print(\"   ‚Ä¢ How to explore and visualize medical data\")\n",
    "print(\"   ‚Ä¢ Multiple ML approaches (classical ML and deep learning)\")\n",
    "print(\"   ‚Ä¢ How to evaluate and compare models\")\n",
    "print(\"   ‚Ä¢ The potential and limitations of ML in healthcare\")\n",
    "print(\"\\nüöÄ What's next:\")\n",
    "print(\"   ‚Ä¢ Apply these techniques to larger datasets\")\n",
    "print(\"   ‚Ä¢ Write your research abstract with confidence\")\n",
    "print(\"   ‚Ä¢ Explore image-based ML for medical imaging\")\n",
    "print(\"   ‚Ä¢ Keep learning and building!\")\n",
    "print(\"\\nüí° Remember: Every expert was once a beginner.\")\n",
    "print(\"   You've got this! Keep going! üí™\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
